Received20March2023,accepted6April2023,dateofpublication13April2023,dateofcurrentversion26April2023.
DigitalObjectIdentifier10.1109/ACCESS.2023.3266804
Emotion Recognition Using Temporally Localized
Emotional Events in EEG With Naturalistic
Context: DENS# Dataset
MOHAMMADASIF ,(GraduateStudentMember,IEEE),SUDHAKARMISHRA ,
MAJITHIATEJASVINODBHAI,ANDUMASHANKERTIWARY ,(SeniorMember,IEEE)
IndianInstituteofInformationTechnologyAllahabad,Allahabad,UttarPradesh211012,India
Correspondingauthors:SudhakarMishra(rs163@iiita.ac.in),MohammadAsif(pse2017001@iiita.ac.in),andUmaShankerTiwary
(ust@iiita.ac.in)
ThisworkwassupportedbytheMinistryofEducation,GovernmentofIndia,fundedbytheacquisitionoftheEEGsystem.
ABSTRACT Emotion recognition using EEG signals is an emerging area of research due to its broad
applicability in Brain-Computer Interfaces. Emotional feelings are hard to stimulate in the lab. Emotions
don’t last long, yet they need enough context to be perceived and felt. However, most EEG-related
emotion databases either suffer from emotionally irrelevant details (due to prolonged duration stimulus)
or have minimal context, which may not elicit enough emotion. We tried to overcome this problem by
designinganexperimentinwhichparticipantswerefreetoreporttheiremotionalfeelingswhilewatching
the emotional stimulus. We called these reported emotional feelings ‘‘Emotional Events’’ in our Dataset
on Emotion with Naturalistic Stimuli (DENS), which has the recorded EEG signals during the emotional
events.Tocompareourdataset,weclassifyemotionaleventsondifferentcombinationsofValence(V)and
Arousal(A) dimensions and compared the results with benchmark datasets of DEAP and SEED. Short-
TimeFourierTransform(STFT)isusedforfeatureextractionandintheclassificationmodelconsistingof
CNN-LSTM hybrid layers. We achieved significantly higher accuracy with our data compared to DEAP
and SEED data. We conclude that having precise information about emotional feelings improves the
classificationaccuracycomparedtolong-durationrecordedEEGsignalswhichmightbecontaminatedby
mind-wandering.Thisdatasetcanbeusedfordetailedanalysisofspecificexperiencedemotionsandrelated
braindynamics.
INDEX TERMS Affective computing, CNN, DEAP, DENS, EEG, emotion dataset, emotion recognition,
LSTM,SEED.
I. INTRODUCTION suitableandconvenientmethodtorecordelectricalactivities
Emotionrecognitionhasbeenachallengingtaskinartificial tomeasurebrainactivitiesasitisanon-invasivemethod,i.e.
intelligence.Severalmethodsareavailableformeasuringthe therearenoscalpelincisions.
participants’ emotions. These methods include behavioural Many studies have already been conducted to measure
changes, subjective experiences self-reported by the par- human affect with the help of EEG and other peripheral
ticipants, peripheral and central nervous system measures, responses [2], [3], [4], [5]. In the previous studies, the
etc[1].Brainactivitiesareamongthemostrobustdimensions focus of the study was to develop a database that is
of detecting human affect, as it is difficult for the users labelled and suitable for emotion detection by intelligent
to manipulate innate brain activity during the process. systems and has contributed to affective computing. There
Accordingly,Electroencephalography(EEG)isconsidereda is a typical method in these studies to elicit emotion in
the participants by presenting them with video clips as
The associate editor coordinating the review of this manuscript and stimuli. In the process of emotion recognition and other
approvingitforpublicationwasJunhuaLi . classification tasks, all the EEG data for that stimulus are
ThisworkislicensedunderaCreativeCommonsAttribution-NonCommercial-NoDerivatives4.0License.
VOLUME11,2023 Formoreinformation,seehttps://creativecommons.org/licenses/by-nc-nd/4.0/ 39913
M.Asifetal.:EmotionRecognitionUsingTemporallyLocalizedEmotionalEvents
to be considered for the classification model, as there is no broader,e.g.,relevance(howmuchthestimulusisrelevantto
information about the precise temporal location at which the participant’s emotional feelings), familiarity (how much
a participant may experience the emotion. Models must theparticipantisfamiliarwiththestimulus)andliking(how
consider all the data presented for that label, which is much the participant liked or disliked the stimulus). Asking
unnecessarily computationally expensive and decreases the participants to report these experiences on a continuous
system’s efficiency by feeding not-so-essential data in the scaleiscommoninsimilarstudies.Sometheoriesdealwith
input. the physiological responses of feeling emotions, e.g., body
In our approach, we have presented a novel method to temperature and heartbeat change [13], [14]. It is obvious
overcome this issue by providing precise information about from the theories that emotion is not a one step process;
the emotion elicitation, self-reported by the participants. instead, it is a combination of physiological responses and
Wecallitan‘EmotionalEvent’.Inthismethod,anadditional other information. Evidence shows that many brain regions
taskisgiventotheparticipantstomentionprecisetemporal are involved during emotion perception [15]. We have also
information by clicking on their computer screens while collectedECGandEMGdataoftheparticipantsalongwith
watchingtheemotionalclipsiftheyfeelsomeemotion.Also, EEGtoconsidertheseparameters.
to the best of our knowledge, there are no EEG affective Emotion recognition through EEG data follows a similar
datasets available for the Indian subcontinent population. pattern as used in various EEG signal analyses. First, the
Hence we tried to reduce this research gap in our work. data is acquired, and some preprocessing is applied to the
WehaveconsideredDEAPdataset[2]andSEEDdataset[3] signal.Thesepreprocessingstepsinvolveremovingartefacts
for comparison. We tried to follow a format similar to the such as ocular activity, muscle activity, and powerline
benchmarkdatasetsandcomparedourdataset’sresultswith interference. Also, downsampling of the signal and band-
thesedatasetsbasedonstatisticalsignificance. pass filtering are used to make data more useful. Various
EEG measures the electrical signals from the scalp with dimensionalityreductiontechniques,suchasICAandPCA,
temporaldetails.DifferentEEGdevicesvarywiththenumber arealsousedtoprunethedatatomakeitfeature-rich.After
of channels of EEG. Thirty-two or fewer EEG channels preprocessing,featuresareextractedfromthesignaltofeed
are especially notable in affective computing research [6]. into the model for the classification task. Different kinds of
A few studies are also available with up to 64 electrodes. featuresareextractedsuchastime-domain(e.g.,event-related
In this work, we used a 128-channel EEG device to detect potential(ERP),high-ordercrossing(HOC),etc.),frequency-
emotions. This EEG cap follows the International 10-10 domain (e.g., power spectral density (PSD), etc.); and
system’sstandards[7]. time-frequency domain (e.g., STFT, wavelet analysis, etc.)
Emotions are complex and challenging to understand as features.
many theories exist about emotions, and there is a lack of a EEGrecordsmulti-frequencynon-stationarybrainsignals
singleconsensustheory[8].Thestudyofemotionshasbeen fromvariouselectrodes.Analyzingthesesignalsischalleng-
an emerging topic that combines multi-disciplines such as ing because of the complex and irregular nature of EEG
psychology, neuroscience, computer science and medicine, signals.Thetime-frequencydomainanalysishasthebenefits
etc. There are different aspects involved in determining of both the time and frequency domains, e.g., better spatial
emotions,suchasbehavioural,psychologicalandphysiolog- andtemporalinformationfromEEGsignals.Onebasictime-
ical aspects, cognitive appraisals, facial expressions, vocal frequency domain feature extraction method is Short-Time
responses,subjectiveexperiences,etc.Thisstudyfocuseson FourierTransform(STFT).STFTisatime-orderedsequence
physiologicalaspectsofemotion,whichareconsideredinto ofspectralestimatesandisoneofthepowerfulandgeneral-
account by the brain signals captured through EEG while purpose signal processing techniques. It has been used in
watching emotional video clips. Further, this study tries to the field of spectral analysis of a signal. The STFT is used
collectacomprehensivelistofsubjectiveexperiencesthrough to compute spectrograms which are used extensively for
aself-assessmentratingattheendofeachclip. signal processing. Spectrograms are visual representations
Manyapproachescouldbeusedtoassesstheparticipants’ of the spectrum of frequencies of a signal with varying
emotional states. Earlier, some basic emotions were used times[16].
thatareuniversallyrecognisedforstudypurposes[9].Later, CNN is the most frequently used architecture for EEG
some theories explained some complex emotions that are analysisandclassificationtasks,andDBNandRNNfollow
a combination of basic emotions [10]. Multi-dimensional it[17].HencewehaveusedacombinationoftheCNNand
theories of emotions are the widely accepted theories for LSTM model. It also helped to compare our dataset with
assessingcoreaffect[11],[12].Accordingtothesetheories, the benchmark datasets in terms of maximum classification
emotions are considered a multi-dimensional array; one accuracy.Usingartificialintelligenceforaffectivecomputing
dimensionisforvalence(experiencingpositiveornegative) provides better learning capabilities to intelligent systems.
and the other for arousal (experiencing the intensity) or Withtheadvancementofcomputingpowerandthedevelop-
dominance (controlling or feeling controlled). A few more mentofeffectiveandadvancedneuralnetworkresearch,the
dimensions are also considered, that make the spectrum trend of using various machine learning and deep learning
39914 VOLUME11,2023
M.Asifetal.:EmotionRecognitionUsingTemporallyLocalizedEmotionalEvents
FIGURE1. CompleteFlowgramoftheExperiment.
techniques has grown within the last few years [18]. This A. EMOTIONALEVENT
workemploysthewidelyusedstate-of-the-artdeeplearning Emotion is a complex phenomenon which is embedded
methodstodetectemotionsfromEEGsignals. withinacontext[21].Moreover,emotionistransientinnature
In this work, we contribute to the affective computing andisnotavailablethroughoutthestimulusduration.Infact,
research by emphasising the importance of considering the morethanoneaspectcouldbeembeddedwithinthestimulus
durationofthesignalencodinginformationaboutemotional context,anddifferentparticipantscanfeelemotionatdiffer-
experience. Emotion duration is the essential component of entpointsoftimeconsideringvariousaspects.However,most
emotion dynamics [19], which is ignored in other datasets. of the datasets recorded to date [2], [3] ignore the transient
Wetakeaccountofemotionduration,which,tothebestofour nature of emotions and provide a single emotional category
knowledge,hadneverbeenconsideredbefore.Bycomparing for the whole stimulus duration. Although the stimulus has
with other datasets using the same stimulus modality, emotional information, it has some non-emotional aspects
we show that better emotion recognition accuracy can be too,whichcouldleadtomind-wanderingactivity.Although
achievedifthetemporalinformationisincorporated. therearesomeattemptstogetcontinuoussubjectivefeedback
Thispaperisorganizedintosixsections.Intheintroduc- onemotionalexperienceandneuralactivity,theexperimental
tion section, we introduced the ongoing trends in affective method involved multiple watching of the stimulus and
computing, EEG emotion analysis and our dataset. In the retrospective collection of emotional experience [22], [23],
next section, we introduced our proposed dataset- DENS, [24], [25], [26]. The retrospective collection depends on
Emotional Events, experimental details (e.g., stimuli, EEG autobiographicalmemoryandcanraisebiasesacrosssubjects
recordings, ratings etc.), preprocessing of the EEG data, depending on their capability to recall [27]. Also, repetitive
its salience features and other datasets used (DEAP and viewing effects can bias the ratings and underlying neural
SEED).Inthemethodologysection,wediscussedthefeature effects [27]. Hence, an experimental paradigm is needed to
extractions, input preprocessing of the extracted features recordtheparticipants’feedbackdynamically,withminimal
for the classifier and deep learning model architecture for distraction during emotion processing and minimizing the
the same. Next, we have the results section, discussing the memory recall biases. In this work, we are introducing
comparison results of the DENS-DEAP and DENS-SEED a novel paradigm in which the time-stamp of emotional
data based on several parameters and also comparing our feelings can be marked online that can be further utilized
results with recent studies. After that, we have a discussion to get the subjective feedback of emotional feelings and
section discussing the results and future aspects. At last, analyzebrainsignalstemporallylocalizedtothefeelingofan
weconcludedouranalysisintheconclusionsection. emotion.Werefertothesetime-stampedemotionalfeelings
as‘‘emotionalevents’’.
II. DATASETONEMOTIONWITHNATURALISTIC
STIMULI(DENS) B. EXPERIMENTALDETAILS
The complete flow diagram of our experiment is given 1) STIMULI
in Fig. 1. We call our dataset ‘Dataset on Emotion with Theselectionofstimulitoinduceparticipants’emotionsalso
NaturalisticStimuli’(abbreviatedasDENS)[20]. playsavitalroleinemotionrecognition.Acarefulselection
VOLUME11,2023 39915
M.Asifetal.:EmotionRecognitionUsingTemporallyLocalizedEmotionalEvents
TABLE1. SelectedstimuliforEEGstudyfromthestimulidatasetwe stimuli. Following are some critical pieces of information
created[28]).Thetimedurationofeachstimulusis60s.StimulusIdsare regardingtheexperiment:
givenforreferencesavailableintheopenscienceframeworkrepository.
• Each participant saw nine (9) emotional stimuli ran-
domly extracted from the set of 16 emotional stimuli
and two (2) non-emotional stimuli as described in the
previoussubsection.
• Whilewatchingtheemotionalfilmstimuli,participants
were instructed to perform a mouse click the moment
theyfeltanyemotion.WecallitanEmotionalEvent.
• At the end of each video stimuli, participants are
provided six self-assessment scales, including valence,
arousal,dominance,liking,familiarity,andrelevance.
• For each click, participants were supposed to select
oneemotionfromtheprovidedlistofemotionspooled
into four quadrants of V-A space (HVHA, LVHA,
LVLA,HVLA)(abbreviations-V:Valence,A:Arousal,
H:High,L:Low) inthedrop-downmenu.Participants
werealsogivenachoicetoentertheemotionalcategory
which suits their emotional experience but is unavail-
of stimuli is critical, and for that, technical validation of able in the provided emotion list. For more details
thevideoclipsiscrucialtoassessiftheintendedemotional seeFig.2.
experienceiselicitedbythestimuli.Wehaveusednaturalistic Before the main experiment begins, participants go
stimuli to elicit emotions in the participants. Naturalistic throughthetrainingphase.Inthetrainingphase,participants
stimuliaredynamicemotionalscenesinwhichmulti-sensory were given instructions about the experiment procedure,
perception is applied. It resembles more to the real-life ratingscaleswereproperlyexplainedbygivingthemasmall
scenario as compared to static and simple stimuli. In our quiz,andalsotheyweretrainedtomouse-clickwhentheyfelt
previouswork,wehavevalidatedasetofmultimediastimuli emotionduringthestimulus.
and created an affective stimuli database [28]. We selected The main experiment consists of the following steps for
16emotionalstimulifromthisdatabasetoperformourEEG eachparticipant:
experiment. The selection criteria for these 16 emotional 1) Baseline Recording: EEG signal was recorded for
stimuliarebasedonthreefactors: 80 seconds while the participant looked at the cross-
1) Ahighprobabilityofelicitingtargetemotions(calcu- markonthescreenandperformednotask.
latedonthebasisofratingsavailable). 2) After baseline recording, one stimulus of 60s was
2) Few stimuli must be available for each emotion presented to the participant. Participants were told to
category. click on the screen when they felt the emotion during
3) SincethisexperimentwasdoneontheIndianpopula- the stimulus. Participants may click more than once
tion,moreemphasiswasgiventoIndianclips. if they felt so but were instructed to refrain from
Besidesthese16emotionalstimuli,wehavevalidated2non- multipleclicksforthesameemotion.EEGsignalswere
emotional stimuli separately. These clips were rated around recordedduringthisphase.
5 mean valence and arousal values (on a scale of 1 to 9). 3) After the stimulus ends, participants go through self-
Thesenon-emotionalclipsincludedtheworld’slongestroad assessment ratings of valence, arousal, dominance,
routesoranimatedhistoryoftheBabylonianera,whichmay liking, familiarity, and relevance. These scales are
not contribute to eliciting emotions. The inclusion of non- explainedindetailinthenextsubsection.
emotionalstimuliwastovalidatetheparticipants’responses 4) At last, participants were supposed to select one
and avoid the long accumulation of the affects during the emotion category for each click (emotional event).
experiment. Tohelptheparticipantsinrecallingabouttheclick,they
For each participant, nine (9) emotional stimuli were werepresentedwiththreeframesaroundtheclick.
selected randomly from the 16 selected emotional stimuli 5) After this, an inter-stimulus interval comes with no
and two (2) non-emotional stimuli. Each stimulus was of time limit. During this interval, participants were
60seconds. givenaquickandeasymathematicalcalculation(e.g.,
Table 1 shows the list of 16 emotional stimuli with the 2+5*2=?). It helps participants to flush their previous
targetemotionsassignedduringthestimulivalidation. emotionalstate.
6) After that, the next stimulus is presented to the
2) EEGRECORDING participant,andsteps1to5arefollowedsimilarlyfor
We recorded the EEG activity of forty participants each stimulus. A total of 11 stimuli (9 emotional and
(23.3±1.25,F=3)whiletheywerewatchingemotionalfilm 2non-emotional)werepresentedtoeachparticipant.
39916 VOLUME11,2023
M.Asifetal.:EmotionRecognitionUsingTemporallyLocalizedEmotionalEvents
FIGURE2. EmotionCategorySelectionScreenforEmotionalEvent(Click):AftertheparticipantsratedallthesixratingscalesofValence,Arousal,
Dominance,Liking,FamiliarityandRelevance,theyareshownthisscreenforemotioncategoryselection.Onthisscreen,threeimageframeswereshown.
Themiddleonebelongstothetimeoftheclick;theleftoneis20framesearlier,andtherightoneis20framesaftertheclick(Pleasenotethatthe
stimulusclipswereshownin30framespersecond).Ithelpsparticipantstorecalleasily.Theyonlyhavetoselectoneemotioncategory.Ifthe
experiencedemotionisnotpresentinthelist,theywerefreetowritetheirown.
3) RATINGS C. PREPROCESSINGANDARTIFACTREMOVAL
Subjectiveratingsareoneofthewell-knownmethodstoeval- OFTHEEEGDATA
uate the personal emotional experience of the participants. The procedure followed to perform the preprocessing is
Emotionalpictures/videosoraudioclipsarepresentedtothe described elsewhere [29]. The critical step which should
participants,andtheyareaskedtoratetheseclipsondifferent be described here includes filtering and artifact removal.
scales based on their personal experiences. These scales We had 128-channel EEG raw data with a sampling rate
include Valence, Arousal, Dominance, Liking, Familiarity of250Hz.TherawsignalisfilteredusingaButterworthfifth-
and Relevance. The rating scales range from 1 to 9 for orderbandpassfilterwiththepassband1-40Hz.Independent
Valence,ArousalandDominance.ForLiking,familiarityand component analysis (ICA) is used to remove artifacts,
Relevance,itrangesfrom1to5.Although,inthisanalysis, includingheartrate,musclemovement,andeyeblink-related
weconsideredonlyvalenceandarousalscales. artifacts.
4) SUMMARYOFTHEEEGSIGNALS D. OTHERDATASETSUSED
As explained above, 465 emotional events were extracted We have used DEAP dataset [2] (a dataset for emotion
from the forty participants in this experiment. All the analysis using EEG, physiological and video signals) and
participants clicked at least one time (average 1.29 times) SEEDdataset[3](Adatasetcollectionforvariouspurposes
duringthestimulus. usingEEGsignals)forcomparingtheresultswithourdataset
Although for each participant and each stimulus, EEG (DENS).
recording is available for the whole stimulus (i.e., for the The DEAP dataset consisted of 40 videos/trials, and for
60s), we have considered the signal for 7 seconds duration eachtrial,thereare40channelsofEEG,includingperipheral
(1secondbeforetheclickand6secondsaftertheclick)for signals, are available, and data is given for each channel.
eachemotionalevent.Wehavetestedforothertimedurations We have used only 32 channels (i.e., discarded peripheral
(e.g., 8s, 9s, up to 10s) but found better results with 7s signals)fortheexperimentasweonlywanttousedatafrom
duration.Therecordinghasasamplingrateof250Hz. thebrainonly.Thisdatawasalreadypreprocessedas128Hz
VOLUME11,2023 39917
M.Asifetal.:EmotionRecognitionUsingTemporallyLocalizedEmotionalEvents
downsampled, bandpass frequency of 4-45 Hz and EOG equation returns the complex Fourier coefficients for the
removed.Foreachtrial,thereare4labelsavailable-Valence kth. These coefficients provide two parameters: phase and
(V),Arousal(A),DominanceandLinking.Wehaveusedonly magnitude.ForSTFT,considertheadditionalparameterhop
V-Aspacefortheexperimentpurpose. size(H),whichisthestepsizeofthewindowtobeshifted.
The SEED dataset was recorded for 15 participants, ωbeasamplingwindowfunctionwhichisω:[0,N-1]→R.
and emotions were presented to the participants into three STFTcanbedefinedas,
categories-positive,negativeandneutralemotions(i.e.,only N−1
valence(V)valueswereused).WehaveusedonlyV-spacein S(m,k):= X x(n+mH)ω(n)e(−i2πkn/N) (2)
theDENSdatasettomatchthenumberofclassesforboththe n=0
datasets.Thedatawasrecordedusing62channels. where m ϵ [0,M] and M is the maximum frame index
mathematically M = (cid:4)L−N(cid:5) . The Short-Time Fourier
H
E. SOMESALIENTFEATURESOFTHEDENS Transform is not only a function of k but also m which is a
To sum up, we are highlighting some key points of our proxytimerepresentation.Here,thefunctionreturnsFourier
dataset- coefficients for the kth proxy frequency at the mth temporal
• Tothebestofourknowledge,thefirsttime,wecreated
bin.
adatasetonEmotionwithNaturalisticStimuli(DENS)
Spectrograms are nothing but the squared magnitude of
andrecordedEEGsignalsfromparticipantsintheIndian
STFTofthesignal.
subcontinent.
χ(m,k):=|S(m,k)|2 (3)
• Stimuli that are used to record EEG data of the
participants are pre-validated on a different set of It is a 2D image where the horizontal axis represents time,
participantsfortheselectedemotioncategories. andtheverticalaxisrepresentsfrequencybins.Thenumber
• Participants were free to select any emotion category, of frequency bins is (framesize / 2) + 1 and the number of
whatevertheyfeltforthestimulifromthegivenlist. time frames is ((size of signal – framesize) / hopsize) + 1.
• We used 128-channel high-density EEG recording for χ(m,k)representsintensityorcolorat(m,k).
higherspatialresolution.
• Emotional Event: Temporal markers are available B. INPUTPREPROCESSINGTOFEEDDATAINTOTHE
for each emotion category when participants feel the CLASSIFIER
emotion,resultinginhighertemporalresolution. Itisessentialtoconvertthedataintoameaningfulformatthat
canbefedintoourclassifiermodel.Asallthreedatasetsare
III. METHODOLOGY
availableindifferentformats,wehaveprovidedinformation
A. FEATUREEXTRACTION
ontheinputpreprocessingforeachdatasetasfollows:
EEG Signals are non-stationary, meaning the signal’s sta-
tistical characteristics change over time. If these signals
1) FORDEAPDATA
are transformed to the frequency domain using Fourier
Each subject in the DEAP dataset is given by a tensor
Transform, it provides the frequency information, which is that is in the form of X ∈ R40×40×8064, representing
averaged over the entire EEG signal. So, information on
40videos,40EEG-channels(includingperipheralchannels),
different frequency events is not analyzed properly. If a
and 8064 EEG data samples for each channel. For labels,
signal is cut into minor segments such that it could be DEAP data provided a matrix in the form of X ∈ R40×4;
considered as stationary and focus on signal properties at a
i.e.,foreachsubject,thereare40videosand4scales.From
particular section which is called a windowing section and
the DEAP dataset, the first 15 subjects are picked. For the
apply Fourier transform on it, it is called as Short-Time
label,weusedtheValence-Arousalspaceanddivideditinto
Fourier Transform (STFT). It will move to the entire signal
fourclasses-HVHA,HVLA,LVLA,LVHA(abbreviations-
lengthandapplyFouriertransformtofindthespectralcontent
H: High, L: Low, V: Valence, A: Arousal). The ratings for
of that section and display the coefficient as a function of
valenceandarousalrangefrom1to9.Hence,weconsidered
bothtimeandfrequency.Itprovidesinsightintothenatureof
ratingsfrom1to5as‘Low’and5to9as‘High’anddivided
thetime-varyingspectralcharacteristicsofthesignal.Before
theV-Aspaceinto4quadrantsaccordingly.
STFT, let’s look at the discrete Fourier transform. Consider
We converted 15 subjects’ data tensor into a matrix
x:[0:L-1]={0,1,...,L−1}→Rbeadiscrete-timesignal of X ∈ R19200×8064 (i.e.,15 subjects × 40 videos ×
whereListhesignallengthwhichisacquiredbyequidistance 32 channels,8064 samples). Moreover, this data was pro-
sample points with respect to the fixed sampling frequency.
cessedforfeatureextractionusingSTFTwithawindowsize
MathematicallyDFTequationis,
of0.5sandanoverlapof0.25sofdatasamples.UsingSTFT,
N−1 we have converted every 8064 sizes of EEG data samples
bx(k)= X x(n).e−i2πnk/N (1) into a spectrogram image size of (33,251), as mentioned in
n=0 the feature extraction section. Then, a hybrid CNN-LSTM
where k ϵ [0, K] and K is the frequency index with respect classifierwasimplementedformulti-classclassificationwith
to Nyquist frequency. N is the duration of the section. The aninputtensorofX∈R33×251×3.
39918 VOLUME11,2023
M.Asifetal.:EmotionRecognitionUsingTemporallyLocalizedEmotionalEvents
FIGURE3. ModelArchitecture:Itisconsistedoftwo2D-convolutionlayerswith3×3kernelsand32filtersand64filtersrespectively,followedbya
maxpoolinglayerfollowedbyadropoutlayerandflatteninglayer.Arepeatvectorlayerofsize4isusedbeforesendingthedatatotheLSTMlayers.
TwoLSTMlayersareusedofsizes256unitsand128unitsrespectively,eachfollowedbyadropoutlayer.Attheend,twodenselayersareusedof
sizes64(followedbyadropoutlayer)and4or3(equalsthenumberoftheoutputclasses).
2) FORSEEDDATA classifier. For the label, we used the same V-A space
SEED dataset contains 45.mat files for 15 subjects for each (HVHA, HVLA, LVLA, LVHA) (abbreviations- H: High,
subject with 3 trials. The label file contains 3 emotional L: Low, V: Valence, A: Arousal) as it was used with the
labels −1 for negative, 0 for neutral, and 1 for positive on DEAP dataset. The ratings for valence and arousal range
the valence scale. After renaming, the labels become 0 for from 1 to 9. Hence, we considered ratings from 1 to 5 as
neutral,1forpositive,and2fornegative.Forclassification, ‘Low’ and 5 to 9 as ‘High’ and divided the V-A space into
we have considered 15.mat files, one trial per subject. Due 4 quadrants accordingly. The dimension of input tensor is
to the different sizes of data length in each channel, the
ofX∈R63×26×3.
first 16000 sample for each data which is the first 80s of TocomparewiththeSEEDdataset,wehaveuseda3-label
data,isconsideredforfurtherprocessing.EEGcapincludes classification since there are only three classes available in
62 channels according to the 10-20 international system. SEED dataset. For the DENS dataset, on the valence scale,
So, 15 subjects, 15 trials, 62-channels, and 16000 EEG ratings below 4.5 are marked as negative (0 labelled), and
data are converted into a tensor of X ∈ R13950×16000 ratings above 5.5 are marked as positive (2 labelled). For
(i.e.,15subjects×15trials×62channels,16000samples)for neutrallabels,intheDENSdataset,wehavenon-emotional
feature extraction. As mentioned in the DEAP dataset files;wehavemarkedneutral(1labelled)forthosefiles’data.
experiment, using STFT with a window size of 0.5s and Thenwiththeclassifier,theinputtensorofX∈R63×26×3 is
overlap of 0.25s, each 16000 EEG data is converted into usedforclassification.
a spectrogram with the shape of (51,319). Then, a hybrid
CNN-LSTM classifier was implemented for multi-class
classificationwithinputtensorshapeX∈R51×319×3. C. MODELARCHITECTUREFORTHE
CLASSIFICATIONTASK
ConvolutionalNeuralNetworks(CNN)andLongShort-Term
3) FORDENSDATA Memory (LSTM) are one of the most widely used deep
For the DENS dataset, we have 465.mat files which learning techniques. CNNs are used to extract meaningful
contain emotional events. All 465 files are picked for the patternsandfeaturesfromthedata.ThekeyelementinCNN
experiment. Each.mat file is a matrix of X ∈ R128×1751, istheconvolutionoperationusingkernelsthatautomatically
where 128 is the number of EEG channels and 1751 is learn the local patterns from data. These local features are
the sample data for each channel. Then we have con- then combined into more complex features when multiple
verted the data tensor of X ∈ R465×128×1751 into the CNN layers are stacked. Filters (i.e, weights trained) in
form of R59520×1751 (i.e.,465 emotional events × this process are also known as feature detector matrices.
128channels,1751samples)forfeatureextractionwithwin- Input data will be convoluted with a filter map by sliding
dow size 0.5s and overlap is 0.25s. After feature extraction, the kernel window. At the same time, LSTM networks can
wehave59520spectrograms,andeachspectrogramisinthe capture the sequential pattern as LSTMs are best suited for
shapeof(63,26). time-series data. LSTMs are designed to work for temporal
TocomparewiththeDEAPdataset,theDENSdatasetwith correlations.Therefore,toexploitthebenefitsofbothCNN
4-labelclassificationisperformedwithahybridCNN-LSTM and LSTM, a hybrid CNN-LSTM architecture is used for
VOLUME11,2023 39919
M.Asifetal.:EmotionRecognitionUsingTemporallyLocalizedEmotionalEvents
FIGURE4. ComparisonofConfusionmatricesforDEAPandDENSdatasetsoverValence-Arousalspace.Thisspaceisdividedintofourclasses
andassignedalabeltoit(0-HVHA,1-HVLA,2-LVHAand3-LVLA).4a:DEAPDataset;4b:DENSDataset.Abbreviationsoftheterms-V:Valence;
A:Arousal;L:Low;H:High.Thecolorbarrepresentsthenumberofsamplesintheclass.
the classification of emotions. The hybrid CNN-LSTM TABLE2. ParameterSettingsfortheModel.
model utilizes the ability of convolutional layers for feature
extractionfromdata,andLSTMlayersareforlong-termand
short-termdependencies.Thesamemodelisusedtocompare
all three datasets. The model classifier and its details are
showninFig.3.
CNNisoftenplacedintheinitiallayersasithelpsinlocal
pattern learning from spectrogram or in general input data.
ThePatternlearningblockconsistsoftwo2D-convolutional
blocks,eachwithakernelsizeof(3×3).Thefeaturemap,
which is the output of convolutional layers, keeps track of The parameter setting for the developed deep learning
the location of the features in the input. A max-Pooling modelismentionedinTable2.
layer is added in between two consecutive convolutional
layers.Apoolinglayerisaddedaftertheconvolutionallayer
IV. RESULTS
to reduce the feature-map dimension; hence it reduces the
The confusion matrix for DEAP, SEED and DENS datasets
computational cost, and the activation function is applied
are shown in Fig. 4 and Fig. 5. In the confusion matrix
to enhance the capability of the model. Rectified Linear
shown,eachcellcontainsdataonthenumberofpopulation.
Unit (ReLU) activation function which has been widely
TheX-axisrepresentsactuallabelsandtheY-axisrepresents
usedtoresilientvanishinggradientproblem.Inbetween,the
predicted labels by the classifier. The diagonal of the
dropoutlayerisusedinsomeplacestoavoidtheoverfitting
matrixrepresentsthecorrectlyidentifiedlabel.Thecolorbar
problem.Theflatteninglayertransformsthesefeaturemaps
representsthenumberofsamplesintheclass.
into one-dimensional vectors. The repeat vector gives extra
dimensionfortheLSTMlayer.Thesequentiallearningblock
consists of 2 LSTM layers which capture the long-term A. COMPARISONBETWEENDEAPANDDENS
temporal dependencies from the feature map extracted by We have used repeated K-Fold cross-validation with
CNN layers. 1st LSTM layer consists of 256 cells with a K = 5 and the number of repeats = 5 so generated
return sequence set to ‘True’ while 2nd LSTM consists of 25accuraciesforDENSandDEAP.Forlabelclassification,
128 cells and as it is the last LSTM layer return sequence we have used V-A space (HVHA, HVLA, LVLA, LVHA).
is‘False’.BetweenLSTMlayers,dropoutlayerswithrate= Comparison between DEAP and DENS is mentioned in
0.2 are added to avoid overfitting issues. Finally, two fully- Table4.ThelossandaccuracygraphsarementionedinFig.8.
connected layers where 1st layer with 64 neurons and 2nd Fig. 6 shows an F1 score comparison between DEAP
layer with the number of classes as neurons are added for and DENS datasets per trial. Using t-test statistical test-
furtherprocessing.Aswehavethemulti-classclassification, ing, the 25 F1 scores of DEAP dataset (M = 95.65%,
theSoftMaxactivationfunctionisusedintheoutputlayeras SD = 0.38%) compared with the 25 F1 scores of DENS
it outputs a vector representing the probability distributions dataset (M = 96.82%, SD = 0.18%), DENS dataset shows
ofalistofpotentialclasses. better results with absolute t (35) = 13.54, p < 0.0001,
39920 VOLUME11,2023
M.Asifetal.:EmotionRecognitionUsingTemporallyLocalizedEmotionalEvents
FIGURE5. ComparisonofConfusionmatricesforSEEDandDENSdatasetsoverValencespace.Thisspaceisdividedintothreeclasses(SEED
datasetprovideddatawiththreeclasses,whileDENSdataisdividedintothreeclassesbasedonthevalenceratingsprovidedbythe
participants)andassignedalabeltoitasfollows:ForSEED:0forneutral,1forpositiveand2fornegative.ForDENS:0forlow-valence(valence
ratingsrangefrom1-4.5),1fornon-emotionaldata(valenceratingsrangefrom4.5-5.5,aswellasneutralcategoriesstimuli)and2for
high-valence(valenceratingsrangesfrom5.5-9).5a:SEEDDataset;5b:DENSDataset.Thecolorbarrepresentsthenumberofsamplesinthe
class.
TABLE3. ComparisonTablewithOtherRecentStudies.
TABLE4. DEAPvsDENSwithmeanF1scores.
TABLE5. SEEDvsDENSwithmeanF1scores.
B. COMPARISONBETWEENSEEDANDDENS
FIGURE6. F1scoresofDEAPvsDENSforallthe25trials. ForSEEDvsDENScomparison,labelclassificationwehave
used 3 labels on the valence scale. Comparison between
d estimate: −13.70 (large), 95 percent confidence interval: SEED and DENS results is mentioned in Table 5. The loss
[−16.51−10.89]. andaccuracygraphsarementionedinFig.8.
VOLUME11,2023 39921
M.Asifetal.:EmotionRecognitionUsingTemporallyLocalizedEmotionalEvents
participant’sfeedbackforthewholedurationofthestimulus
might not be correctly capturing emotional experience (in
particular)[36].Hence,itisimportanttoknowtheduration
of the emotional experience without compromising the
ecologicalvalidityofthestimuli.
The main idea behind this work is that if we can capture
the temporal marker of emotional experience within a
real-life resembling environment, we might achieve better
accuracy than the accuracy achieved to date with other
datasets lacking information about time. Although, due to
thelimitednumberofsubjects,wedidn’tgoforthesubject-
independentclassificationfornow.Though,infuture,wewill
FIGURE7. F1scoresofSEEDvsDENSforallthe25trials.
becollectingmoredatatomitigatethislimitation.
In our results, we observed that the same hybrid deep
Fig. 7 shows an F1 score comparison between SEED
learning model on our dataset not only outperformed other
and DENS datasets per trial. Using t-test statistical test-
datasets,includingbenchmarkdatasetslikeDEAPandSEED
ing, the 25 F1 score of SEED dataset (M = 95.65%,
butalsoachievedabetterresultwhencomparingwithother
SD=0.37%)comparedwiththe25F1scoreofDENSdataset
noteworthy relevant studies (see Table 3). Classification
(M = 97.68%, SD = 0.13%), DENS dataset shows better
of DEAP data into four labels, including HVHA, LVHA,
resultswithabsolutet(31)=25.466,p < 0.0001,Cohen’s
LVLA, and HVLA, resulted in 95.65% mean accuracy.
d estimate: −11.37 (large), 95 percent confidence interval:
At the same time, the classification of DENS data into
[−13.73−9.02].
four labels resulted in 96.82% mean accuracy. Similarly,
the classification of SEED data into three labels resulted
C. COMPARISONWITHOTHERRECENTSTUDIES in 95.65% mean accuracy while DENS data resulted in
We have included some other recent studies and given a 97.68%meanaccuracy.Thesignificancetestingshowedthat
comparative table for their results in Table 3. The studies evenwithmultipleiterations,theclassificationaccuracywas
consist of CNN-RNN Hybrid models, R2G-STNN model significantlyhigherforourdata.
that is based on regional to global BiLSTM with Attention To the date, most of the work on emotion recognition
layer,Attention-basedCNN-RNNHybridmodel(ACRNN), applieddifferentshallowmachinelearninganddeeplearning
BiDCNNthatisBi-hemisphereDiscrepancyCNNmodeland techniques using many different configurations of input
ECLGCNNthatisafusionmodelofGraphCNNandLSTM dataincluding,spectrogram,rawsignals,statisticalfeatures,
model. variational mode decomposition (VMD), empirical mode
decomposition (EMD), functional connectivity based fea-
V. DISCUSSION tures, fractal features and so on. However, still, the recog-
In this work, we captured emotional experiences within the nition of emotion from EEG stands as a problem. Most of
ecologically valid naturalistic environment with a precise theworksonemotionrecognitionhaveusedsomebenchmark
temporal marker than any study to date. As per recent datasets,includingDEAP,SEED,AMIGO,MAHNOB-HCI
theories,emotionalexperienceisaconstructingphenomenon andsoon.Though,mostoftheemotionclassificationworks
which involves networks of the brain, including the default revolvearoundDEAPandSEEDdatasets[2].
mode network, salience network, and fronto-parietal net- In[37],emotionalstatesareclassifiedbymeansofEEG-
work. These networks are not specific to emotional experi- based functional connectivity patterns. Forty participants
ences. In fact, these networks are domain-general networks viewedaudio-visualfilmclipstoevokeneutral,positive(one
which are involved in perception (in general). Though, amusing and one surprising) or negative (one fear and one
the connectivity among these networks might not be the disgust) emotions. Correlation, coherence, and phase syn-
same in different perceptions which is apparently shown in chronizationareusedforestimatingtheconnectivityindices.
our previous work [35]. In addition, different from normal They stated significant differences among emotional states.
perception, emotional experiences involve changes in body A maximum classification rate of 82% was reported when
physiology[29].Puttingtogethertheabove-mentionedideas the phase synchronization index was used for connectivity
from recent results hints that the emotional experiences can measure.
be easily confused with other perceptions, which might not The classes considered in the study are elementary.
beanemotionalexperience. We suspect that with the increasing number of emotional
Oneofthemajorconcernsisthemind-wanderingactivity classes, which includes not only basic classes but complex
while using the film stimuli. In the previous research, the emotions as well, taking the long-duration signal without a
whole stimulus is considered to elicit a single emotional temporal marker may not be able to categorize emotional
experience. And the duration of the stimulus varied from classes.Thereasonisthattherearefewerchancesforamovie
seconds to minutes. Research shows that averaging the stimulus to have a positive as well as a negative emotional
39922 VOLUME11,2023
M.Asifetal.:EmotionRecognitionUsingTemporallyLocalizedEmotionalEvents
FIGURE8. LossandAccuracyGraphsforAllthedatasetsUsed.
VOLUME11,2023 39923
M.Asifetal.:EmotionRecognitionUsingTemporallyLocalizedEmotionalEvents
experienceinthesamestimuli,butitiscertainlypossiblethat [11] J.A.Russell,‘‘Coreaffectandthepsychologicalconstructionofemotion,’’
itcanhavemorethanonepositiveormorethanonenegative Psychol.Rev.,vol.110,no.1,p.145,2003.
[12] G. K. Verma and U. S. Tiwary, ‘‘Multimodal fusion framework: A
feelinginthemovie.
multiresolutionapproachforemotionclassificationandrecognitionfrom
physiologicalsignals,’’NeuroImage,vol.102,pp.162–172,Nov.2014.
VI. CONCLUSION [13] J.Dewey,‘‘Thetheoryofemotion:I:Emotionalattitudes,’’Psychol.Rev.,
The work presented in this article is based on the concept vol.1,no.6,pp.553–569,Nov.1894.
[14] W. B. Cannon, ‘‘The James–Lange theory of emotions: A critical
that emotion is a short-lived phenomenon which might last
examinationandanalternativetheory,’’Amer.J.Psychol.,vol.39,nos.1–4,
for very few seconds. Hence, using long-duration EEG p.106,Dec.1927.
signals recorded during emotional stimulus watching might [15] T. Dalgleish, ‘‘The emotional brain,’’ Nature Rev. Neurosci., vol. 5,
pp.583–589,Jul.2004.
not contain emotional information for the whole duration.
[16] J. Allen, ‘‘Applications of the short time Fourier transform to speech
Therefore, we hypothesized that using only the duration of
processing and spectral analysis,’’ in Proc. IEEE Int. Conf. Acoust.,
the signal where an emotional event is reported without Speech, Signal Process. (ICASSP), May 1982, pp.1012–1015, doi:
compromising the ecological validity of the stimuli will 10.1109/ICASSP.1982.1171703.
[17] Y. Roy, H. Banville, I. Albuquerque, A. Gramfort, T. H. Falk, and
containmoreemotionalinformation.Totestthehypothesis,
J.Faubert,‘‘Deeplearning-basedelectroencephalographyanalysis:Asys-
we designed an EEG experiment which uniquely marks the tematicreview,’’J.NeuralEng.,vol.16,no.5,Oct.2019,Art.no.051001.
durationoftheemotionaleventinthecontinuousrecordingof [18] F.Chollet,DeepLearningWithPython.NewYork,NY,USA:Simonand
Schuster,2021.
brainwavesusingEEG.Weperformeddeeplearninganalysis
[19] R. J. Davidson, ‘‘Comment: Affective chronometry has come of age,’’
usingahybridCNNandLSTMmodelandfoundresultsthat
EmotionRev.,vol.7,no.4,pp.368–370,Oct.2015.
significantly favoured our hypothesis. In this work, we saw [20] S.Mishra,M.Asif,N.Srinivasan,andU.M.Tiwary,‘‘Datasetonemotion
the problem with a different aspect which has not attracted withnaturalisticstimuli(DENS)onIndiansamples,’’bioRxiv,pp.1–11,
Dec. 2022. [Online]. Available: https://www.biorxiv.org/content/early/
theattentionoftheresearcher.Wesuggestthatfutureresearch
2022/12/31/2021.08.04.455041,doi:10.1101/2021.08.04.455041.
onemotionrecognitionshouldadaptourapproachtocollect [21] L. F. Barrett, B. Mesquita, and M. Gendron, ‘‘Context in emotion
more such kinds of data so that emotion recognition using perception,’’CurrentDirectionsPsychol.Sci.,vol.20,no.5,pp.286–290,
Oct.2011,doi:10.1177/0963721411422522.
EEG can go beyond the emotions only and move towards
[22] J.Zhang,W.Cheng,Z.Liu,K.Zhang,X.Lei,Y.Yao,B.Becker,Y.Liu,
recognizingandanalyzingmorecomplexemotions. K.M.Kendrick,G.Lu,andJ.Feng,‘‘Neural,electrophysiologicaland
anatomicalbasisofbrain-networkvariabilityanditscharacteristicchanges
inmentaldisorders,’’Brain,vol.139,no.8,pp.2307–2321,Aug.2016.
ACKNOWLEDGMENT
[23] C. B. Young, G. Raz, D. Everaerd, C. F. Beckmann, I. Tendolkar,
#DatasetonEmotionwithNaturalisticStimuli.Availability
T.Hendler,G.Fernández,andE.J.Hermans,‘‘Dynamicshiftsinlarge-
at (https://openneuro.org/datasets/ds003751). (Mohammad scalebrainnetworkbalanceasafunctionofarousal,’’J.Neurosci.,vol.37,
AsifandSudhakarMishracontributedequallytothiswork.) no.2,pp.281–290,Jan.2017.
[24] G. Raz, A. Touroutoglou, C. Wilson-Mendenhall, G. Gilam, T. Lin,
T.Gonen,Y.Jacob,S.Atzil,R.Admon,M.Bleich-Cohen,A.Maron-Katz,
REFERENCES T. Hendler, and L. F. Barrett, ‘‘Functional connectivity dynamics
during film viewing reveal common networks for different emotional
[1] I. B. Mauss and M. D. Robinson, ‘‘Measures of emotion: A review,’’
experiences,’’ Cogn., Affect., Behav. Neurosci., vol. 16, pp. 709–723,
CognitionEmotion,vol.23,no.2,pp.209–237,Feb.2009.
May2016.
[2] S.Koelstra,C.Muhl,M.Soleymani,J.-S.Lee,A.Yazdani,T.Ebrahimi,
[25] M. E. Sachs, A. Habibi, A. Damasio, and J. T. Kaplan, ‘‘Dynamic
T.Pun,A.Nijholt,andI.Patras,‘‘DEAP:Adatabaseforemotionanalysis;
intersubject neural synchronization reflects affective responses to sad
usingphysiologicalsignals,’’IEEETrans.Affect.Comput.,vol.3,no.1,
music,’’NeuroImage,vol.218,Sep.2020,Art.no.116512.
pp.18–31,Jan./Mar.2012.
[3] W.-L.ZhengandB.-L.Lu,‘‘Investigatingcriticalfrequencybandsand [26] G.Lettieri,G.Handjaras,F.Setti,E.M.Cappello,V.Bruno,M.Diano,
channelsforEEG-basedemotionrecognitionwithdeepneuralnetworks,’’ A.Leo,E.Ricciardi,P.Pietrini,andL.Cecchetti,‘‘Defaultandcontrol
IEEETrans.Auto.MentalDevelop.,vol.7,no.3,pp.162–175,Sep.2015, network connectivity dynamics track the stream of affect at multiple
doi:10.1109/TAMD.2015.2431497.
timescales,’’SocialCogn.Affect.Neurosci.,vol.17,no.5,pp.461–469,
May2022.
[4] S.KatsigiannisandN.Ramzan,‘‘DREAMER:Adatabaseforemotion
recognitionthroughEEGandECGsignalsfromwirelesslow-costoff- [27] M.Andric,S.Goldin-Meadow,S.L.Small,andU.Hasson,‘‘Repeated
the-shelf devices,’’ IEEE J. Biomed. Health Informat., vol. 22, no. 1, movie viewings produce similar local activity patterns but different
pp.98–107,Jan.2018,doi:10.1109/JBHI.2017.2688239. networkconfigurations,’’NeuroImage,vol.142,pp.613–627,Nov.2016.
[5] J.A.Miranda-Correa,M.K.Abadi,N.Sebe,andI.Patras,‘‘AMIGOS: [28] S.Mishra,N.Srinivasan,M.Asif,andU.M.Tiwary.(Nov.2021).Affective
A dataset for affect, personality and mood research on individuals and FilmDatasetFromIndia(AFDI):CreationandValidationWithanIndian
groups,’’ IEEE Trans. Affect. Comput., vol. 12, no. 2, pp.479–493, Sample.[Online].Available:https://psyarxiv.com/yajsk
Apr.2021,doi:10.1109/TAFFC.2018.2884461. [29] S.Mishra,N.Srinivasan,andU.M.Tiwary,‘‘Cardiac–braindynamics
[6] M.M.Rahman,A.K.Sarkar,M.A.Hossain,M.S.Hossain,M.R.Islam, dependoncontextfamiliarityandtheirinteractionpredictsexperienceof
M.B.Hossain,J.M.W.Quinn,andM.A.Moni,‘‘Recognitionofhuman emotionalarousal,’’BrainSci.vol.12,no.12,p.702,2022.
emotionsusingEEGsignals:Areview,’’Comput.Biol.Med.,vol.136, [30] X.Li,D.Song,P.Zhang,G.Yu,Y.Hou,andB.Hu,‘‘Emotionrecognition
Sep.2021,Art.no.104696. from multi-channel EEG data through convolutional recurrent neural
[7] P.LuuandT.Ferree,‘‘Determinationofthehydrocelgeodesicsensornets’ network,’’inProc.IEEEInt.Conf.Bioinf.Biomed.(BIBM),Dec.2016,
average electrode positions and their 10–10 international equivalents,’’ pp.352–359.
ElectricalGeodesics,Inc.,Eugene,OR,USA,Tech.Note,2005,pp.1–11. [31] Y.Li,W.Zheng,L.Wang,Y.Zong,andZ.Cui,‘‘Fromregionaltoglobal
[8] C.E.Izard,HumanEmotions.NewYork,NY,USA:Springer,2013. brain: A novel hierarchical spatial–temporal neural network model for
[9] C.E.Izard,‘‘Basicemotions,naturalkinds,emotionschemas,andanew EEGemotionrecognition,’’IEEETrans.Affect.Comput.,vol.13,no.2,
paradigm,’’Perspect.Psychol.Sci.,vol.2,no.3,pp.260–280,Sep.2007. pp.568–578,Apr.2022.
[10] R. Plutchik, ‘‘A general psychoevolutionary theory of emotion,’’ in [32] W.Tao,C.Li,R.Song,J.Cheng,Y.Liu,F.Wan,andX.Chen,‘‘EEG-
Theories of Emotion. Amsterdam, The Netherlands: Elsevier, 1980, basedemotionrecognitionviachannel-wiseattentionandselfattention,’’
pp.3–33. IEEETrans.Affect.Comput.,vol.14,no.1,pp.382–393,Jan.2023.
39924 VOLUME11,2023
M.Asifetal.:EmotionRecognitionUsingTemporallyLocalizedEmotionalEvents
[33] D.Huang,S.Chen,C.Liu,L.Zheng,Z.Tian,andD.Jiang,‘‘Differences MAJITHIA TEJAS VINODBHAI was born in
first in asymmetric brain: A bi-hemisphere discrepancy convolutional Jamnagar, Gujarat, India, in June 1995. He is
neuralnetworkforEEGemotionrecognition,’’Neurocomputing,vol.448, currentlypursuingtheM.Tech.degreeinITwitha
pp.140–151,Aug.2021. specializationinmachinelearningandintelligent
[34] Y.Yin,X.Zheng,B.Hu,Y.Zhang,andX.Cui,‘‘EEGemotionrecognition systems with the Indian Institute of Information
usingfusionmodelofgraphconvolutionalneuralnetworksandLSTM,’’ Technology Allahabad, Allahabad. His research
Appl.SoftComput.,vol.100,Mar.2021,Art.no.106954.
interestsincludemachinelearning,deeplearning,
[35] S. Mishra, N. Srinivasan, and U. S. Tiwary, ‘‘Dynamic functional
anditsapplicationincognitivescience.Hehastwo
connectivityofemotionprocessinginbetabandwithnaturalisticemotion
yearsofworkexperienceasaSoftwareEngineer
stimuli,’’BrainSci.,vol.12,no.8,p.1106,Aug.2022.
withTechMahindraLtd.,Pune,India.
[36] H.Saarimäki,‘‘Naturalisticstimuliinaffectiveneuroimaging:Areview,’’
FrontiersHum.Neurosci.,vol.15,p.318,Jun.2021.
[37] Y.-Y.LeeandS.Hsieh,‘‘Classifyingdifferentemotionalstatesbymeans
ofEEG-basedfunctionalconnectivitypatterns,’’PLoSONE,vol.9,no.4,
Apr.2014,Art.no.e95415.
MOHAMMADASIF(GraduateStudentMember,
IEEE)receivedthebachelor’sdegreeincomputer
science and the master’s degree in cognitive
science and in information technology (special-
izing in software engineering). He is currently
a Research Scholar with the Indian Institute of UMA SHANKER TIWARY (Senior Member,
Information Technology Allahabad, Allahabad. IEEE)receivedthePh.D.degreefromtheDepart-
Hisresearchinterestincludesaffectivecomputing. ment of Electronics Engineering, Institute of
Heisalsoworkingonemotionrecognitionusing Technology,BanarasHinduUniversity,Varanasi,
brain signals. He is using EEG for emotion India, in 1991. He was a Lecturer with the
detection using validated stimuli. He is also working on deep learning Department of Electronics and Communication,
architectures. J.K.InstituteofAppliedPhysicsandTechnology,
UniversityofAllahabad,fromSeptember1988to
March 1992. From March 1992 to June 2002,
he was a Reader in computer science with the
J.K.InstituteofAppliedPhysicsandTechnology,UniversityofAllahabad.
SUDHAKAR MISHRA received the master’s HewasalsoaVisitingScientistwiththeDepartmentofComputerScience
degree in human–computer interaction from andEngineering,IITKanpur,fromDecember1995toJuly1996.Hewas
the Indian Institute of Information Technol- anAssociateProfessorwiththeIndianInstituteofInformationTechnology
ogy Allahabad, Prayagraj, India, where he is Allahabad,Allahabad,India,fromJuly2002toDecember2006,wherehe
currently pursuing the Graduate degree. He is hasbeenaProfessorwiththeDepartmentofInformationTechnology,since
alsodoingresearchonspatio-temporaldynamics December2006.Heisholdingresearchandteachingexperienceformore
of emotions. He has conducted two important than 30 years, in which he is very much involved in image processing,
experimentsonIndiansamples,whichresultsin computervision,medicalimageprocessing,patternrecognitionandscript
theavailabilityofstimulidataset(validatedonan analysis,digitalsignalprocessing,speechandlanguageprocessing,wavelet
Indiansample)andtheavailabilityofEEGdataset transforms, soft computing and fuzzy logic, neurocomputing and soft-
with unique information about the time of emotional experience during computers, speech-driven computers, natural language processing, brain
watchingthenaturalisticmultimediastimuli.HeisamemberoftheSociety simulation,cognitivescience,andaffectivecomputing.
forNeuroscience.
VOLUME11,2023 39925
